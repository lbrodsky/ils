{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___\n",
    "\n",
    "# [ Image and Laboratory Spectroscopy  ]\n",
    "\n",
    "**Department of Applied Geoinformatics and Carthography, Charles University** \n",
    "\n",
    "*Lukas Brodsky lukas.brodsky@natur.cuni.cz*\n",
    "\n",
    "    \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil VNIR spectroscopy with PLSR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to build a regression model using the PLSR in Python for Soil Organic Carbon prediction from spectroscopy data. \n",
    "\n",
    "It covers: \n",
    "\n",
    "1. Introduction to the difference PLS-regression; \n",
    "2. Present the basic code for PLSR; \n",
    "3. Discuss the data we want to analyze (Soil VNIR spectra to predict Soil Organic Carbon content); \n",
    "4. Build the model of using a cross-validation approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## PLSR\n",
    "PLSR, acronym of Partial Least Squares, is a widespread regression technique used to analyze near-infrared spectroscopy data to predict continuous variable. Another, linked technique, is PCR (Principal COmponent Regression), which is simply a regression model built using a number of principal components derived using Principal Component Analysis (PCA). Yet the PCR is simple, however it does not take into account anything other than the components. It does not take into account our labels (reference) y. It solely depends on the variation in the input independent variables (X). That is obviously not optimal, and PLSR is a way to improve. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between PCR and PLSR\n",
    "\n",
    "Both PLS and PCR perform multiple linear regression, that is they build a linear model, y = Xb + e; \n",
    "where X are the predictor variables and y is the response variable. \n",
    "In VNIR analysis, the X are spectral values (e.g. covering the range of 350 to 2500 nm), y is the quantity - or quantities - we want to calibrate for (in our case soil / geology continuous property of interest). Finally e is an error (nois in the data).\n",
    "\n",
    "The matrix X of the spectral values contains highly correlated data. The correlation is unrelated to our soil / geology property of interest, and it may obscure the variations we want to measure. Both PCR and PLSR will remove  the correlation.\n",
    "\n",
    "In the PCR, the set of measuremd X is transformed into equivalent $X'=XW$ by a linear transformation $W$, such that all the new 'spectra' (which are the principal components) are linear independent. In statistics $X'$ is called the **factor scores**.\n",
    "\n",
    "The linear transformation in PCR is such that it minimises the covariance between the diffrent rows of $X'$. That means this process only uses the spectral data, not the response values.\n",
    "\n",
    "In the PLSR instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the y space. PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases (unless it is regularized). \n",
    "\n",
    "PLSR is based on finding a similar linear transformation, but accomplishes the same task by maximising the covariance between $Y$ and $X'$. In other words, PLSR takes into account both spectra and response values and in doing so will improve on some of the limitation on PCR. For these reasons PLSR is typically prefered analytical tool in the soil and geology spectroscopy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLSR in Python Scikit-learn\n",
    "`sklearn` already has got a PLSR package. \n",
    "**API:**\n",
    "\n",
    "* `from sklearn.cross_decomposition import PLSRegression` \n",
    "\n",
    "* `PLSRegression(n_components=2, *, scale=True, max_iter=500, tol=1e-06, copy=True)`\n",
    "\n",
    "where \n",
    "\n",
    "   **n_components** (int) is the numbner of components to keep for the model (user devined, however, default=2)\n",
    "    It should be in [1, min(n_samples, n_features, n_targets)].\n",
    "\n",
    "   **scale** (bool) whether to scale X and Y (default=True). \n",
    "\n",
    "   **max_iter** (int) is the maximum number of iterations (default=500). \n",
    "\n",
    "   **tol** (float), tolerance used as convergence criteria (default=1e-06). The algorithm stops whenever the squared norm of u_i - u_{i-1} is less than tol, where u corresponds to the left singular vector.\n",
    "\n",
    "   **copy** (bool)  whether to copy X and Y in fit before applying centering, and potentially scaling. If False, these operations will be done inplace, modifying both arrays. (default=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Basic PLSR practice**\n",
    "\n",
    "1/ first run the model for several number of components and define the `n_components` we want to keep in our PLS-regression; \n",
    "\n",
    "\n",
    "2/ once the PLS object is defined, we fit the regression to the data `x` (the preditor) and `y` (the known response). \n",
    "\n",
    "3/ use the model to run a cross-validation experiment (e.g. 10 fold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os \n",
    "from sys import stdout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Soil Spectral Library \n",
    "Soil spectroscopy is the measurement of light absorption when light in the visible, near infrared or mid infrared (Vis–NIR–MIR) regions of the electromagnetic spectrum is applied to a soil surface. The proportion of the incident radiation reflected by soil is sensed through Vis–NIR–MIR reflectance spectroscopy. These characteristic spectra (see Fig. below) can then be used to estimate numerous soil attributes including: minerals, organic compounds and water.\n",
    "\n",
    "Read more on the soil spectroscopy web page: https://soilspectroscopy.org \n",
    "\n",
    "Here we use a small subset of the soil spectra collected over the area of the Czech Republic for demonstration purposes coming from Eurostat LUCAS data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and explore the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './'\n",
    "fn = 'ossl_vnir_soil_organic_carbon_subset.csv'\n",
    "data = pd.read_csv(os.path.join(data_path, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first column is original dat id \n",
    "data.rename(columns={list(data)[0]:'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the content\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which column strats the spectral data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, 3:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Than define your arrayx of X and y for the PLSR modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['oc_usda.calc_wpct'].values\n",
    "X = data.values[:, 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the SOC data distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Howabout histogram asymetry? \n",
    "# Can we improve? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple square root \n",
    "# if used, the data needs treatment after the model prediction to get the same scale for the SOC \n",
    "y_sqrt = np.sqrt(y)\n",
    "pd.DataFrame(y_sqrt).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot soil spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame stors the spectral values as separate columns. It starts at 452 nm and ends with 2500 nm,  \n",
    "# while the OSSL uses step 2 nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the wavelength of the spectral records\n",
    "wl = np.arange(452, 2501, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one spectra and plot the soil spectral curve\n",
    "sel_spectra = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('ggplot'):\n",
    "    plt.plot(wl, X[1:5, :].T)\n",
    "    plt.xlabel(\"Wavelengths (nm)\")\n",
    "    plt.ylabel(\"Reflectance (%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If required, data can be easily sorted by PCA and corrected with multiplicative scatter correction; \n",
    "# Another yet simple way to enhance the spectra is to perform second derivative; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first smooth the spectra with Savitzky–Golay filter;  \n",
    "# to increase the precision of the data without distorting the signal tendency\n",
    "# by fitting successive sub-sets of adjacent data points with a low-degree polynomial by OLS; \n",
    "X_sg = savgol_filter(X, window_length = 17, polyorder=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and calculate the second derivative \n",
    "X_d2 = savgol_filter(X_sg, window_length = 17, polyorder=2, deriv=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and see\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "with plt.style.context('ggplot'):\n",
    "    plt.plot(wl, X_d2[1, :].T)\n",
    "    plt.xlabel(\"Wavelengths (nm)\")\n",
    "    plt.ylabel(\"Reflectance 2nd_drv\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PLSR model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find optimal number of components \n",
    "\n",
    "number_cv = 10\n",
    "\n",
    "def optimise_pls_cv(X, y, n_comp):\n",
    "    # PLSR object\n",
    "    plsr = PLSRegression(n_components=n_comp)\n",
    "\n",
    "    # cross-validation\n",
    "    y_cv = cross_val_predict(plsr, X, y, cv=number_cv)\n",
    "\n",
    "    # calculate scores\n",
    "    r2 = r2_score(y, y_cv)\n",
    "    mse = mean_squared_error(y, y_cv)\n",
    "    rpd = y.std()/np.sqrt(mse)\n",
    "    \n",
    "    return (y_cv, r2, mse, rpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model quality evaluation** \n",
    "\n",
    "R2 ... coefficient of determination \n",
    "\n",
    "MSE ... mean squared error, the average squared difference between the estimated values and the actual value\n",
    "\n",
    "RPD ... residual prediction deviation;  RPD is calculated as the ratio of the standard deviation of reference (training data set) soil property values to the RMSE of prediction (Viscarra Rossel, 2007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Three scenarios of model quality** \n",
    "\n",
    "Scenario A: moderate quality PLSR model with R2 ≥ 0.6, RPD ≥ 1.5\n",
    "\n",
    "Scenario B: good quality PLSR model with R2 ≥ 0.7 and RPD ≥ 1.5\n",
    "\n",
    "Scenario C: the best quality PLSR model from the given data set with R ≥ 0.8 and RPD ≥ 2.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "# test with up to selected maximun number of components\n",
    "# more components require more calculations! \n",
    "max_comp = 20\n",
    "\n",
    "# select your pre-processed input data (X, y) \n",
    "prep_X = X\n",
    "prep_y = y_sqrt\n",
    "\n",
    "# run the 'grid search' with cross-validation\n",
    "r2s =  []\n",
    "mses = []\n",
    "rpds = []\n",
    "xticks = np.arange(1, max_comp + 1)\n",
    "for n_comp in xticks:\n",
    "    y_cv, r2, mse, rpd = optimise_pls_cv(prep_X, prep_y, n_comp)\n",
    "    r2s.append(r2)\n",
    "    mses.append(mse)\n",
    "    rpds.append(rpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the calcualted metrics to see how the model performs for the range of components\n",
    "def plot_metrics(vals, ylabel, objective):\n",
    "    with plt.style.context('ggplot'):\n",
    "        plt.plot(xticks, np.array(vals), '-o', color='blue', mfc='blue')\n",
    "        if objective=='min':\n",
    "            idx = np.argmin(vals)\n",
    "        else:\n",
    "            idx = np.argmax(vals)\n",
    "        plt.plot(xticks[idx], np.array(vals)[idx], 'P', ms=10, mfc='red')\n",
    "\n",
    "        plt.xlabel('Number of PLSR components')\n",
    "        plt.xticks = xticks\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title('PLSR')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(mses, 'MSE', 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(rpds, 'RPD', 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(r2s, 'R2', 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that all the metrics confirm that X components is the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place here your (model) selection\n",
    "n_components = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caculate the predictions and quality metrics for the selected model\n",
    "y_cv, r2, mse, rpd = optimise_pls_cv(prep_X, prep_y, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our model quality is: R2=%0.4f, MSE=%0.4f, RPD=%0.4f' %(r2, mse, rpd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it reasonable result? \n",
    "# Compare it with existing literature publications! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you scaled the original SOC value, you should rescale back \n",
    "y_cv_ = y_cv ** 2 \n",
    "prep_y_ = prep_y ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scatterplot with the seelcted regression model \n",
    "plt.figure(figsize=(6, 6))\n",
    "with plt.style.context('ggplot'):\n",
    "    plt.scatter(prep_y_, y_cv_, color='red', alpha=0.7)\n",
    "    z = np.polyfit(prep_y_, y_cv_, 1)\n",
    "    plt.plot(prep_y_, np.polyval(z, prep_y_), color='blue', \n",
    "             linewidth=0.5, linestyle='--', \n",
    "             label='Predicted regression line')\n",
    "    plt.xlabel('Actual SOC (%)')\n",
    "    plt.ylabel('Predicted SOC (%)')\n",
    "    plt.legend()\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks for students\n",
    "\n",
    "* 1/ Split the input data set into train and test set to properly evlaute the model!\n",
    "* 2/ Evluate what is the histogram asymatry influence on the model quality; \n",
    "* 3/ Test several differnt spectra pre-treatments; \n",
    "* 4/ Collect your own spectra of given soil sample with FieldSpec and predict its soil organic carbon concentration (%) using the above developed predictive model! \n",
    "* 5/ Optionally, replace the PLSR model with non-linear model, any; "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
